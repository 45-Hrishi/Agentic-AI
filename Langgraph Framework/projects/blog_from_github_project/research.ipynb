{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Blog Writer From Github Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project flow = START --> reponame --> code + docstring --> search for content related to docstring --> search on web for any information which is not clear --> generate the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_groq import ChatGroq\n",
    "from typing_extensions import TypedDict,Literal,List\n",
    "from pydantic import BaseModel,Field\n",
    "import os\n",
    "import requests\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the environment variables\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the llm model\n",
    "llm = ChatGroq(model=\"qwen-2.5-32b\",temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_github_code(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_content = fetch_github_code(url=\"https://raw.githubusercontent.com/45-Hrishi/Agentic-AI/refs/heads/main/Langgraph%20Framework/testing/testing.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from langchain.chains import LLMChain\\r\\nfrom langchain.memory import ConversationBufferMemory\\r\\nfrom langchain.prompts import PromptTemplate\\r\\nfrom langchain.llms import OpenAI\\r\\nfrom langchain.tools import GoogleSearchAPIWrapper\\r\\n\\r\\ndef create_research_chain():\\r\\n    \"\"\"\\r\\n    Creates an LLMChain with conversational memory and web search capability.\\r\\n\\r\\n    This function sets up a conversational agent that can remember past interactions\\r\\n    and perform web searches to retrieve the latest research information.\\r\\n\\r\\n    Returns:\\r\\n        LLMChain: A chain object that processes user input, maintains conversation history,\\r\\n                  and fetches web search results.\\r\\n    \"\"\"\\r\\n    # Define the prompt template with placeholders for chat history and user input\\r\\n    template = \"\"\"You are a research assistant chatbot having a conversation with a human.\\r\\n\\r\\n    {chat_history}\\r\\n    Human: {human_input}\\r\\n    Chatbot:\"\"\"\\r\\n\\r\\n    # Initialize the prompt with the specified input variables and template\\r\\n    prompt = PromptTemplate(\\r\\n        input_variables=[\"chat_history\", \"human_input\"], template=template\\r\\n    )\\r\\n\\r\\n    # Initialize the conversation memory to keep track of the chat history\\r\\n    memory = ConversationBufferMemory(memory_key=\"chat_history\")\\r\\n\\r\\n    # Initialize the language model (LLM) you want to use (e.g., OpenAI\\'s GPT-3)\\r\\n    llm = OpenAI()\\r\\n\\r\\n    # Initialize the web search tool (e.g., Google Search)\\r\\n    search = GoogleSearchAPIWrapper()\\r\\n\\r\\n    # Create the LLMChain with the LLM, prompt, memory, and web search tool\\r\\n    llm_chain = LLMChain(\\r\\n        llm=llm,\\r\\n        prompt=prompt,\\r\\n        memory=memory,\\r\\n        tools=[search],  # Add the search tool to the chain\\r\\n        verbose=True,\\r\\n    )\\r\\n\\r\\n    return llm_chain\\r\\n\\r\\n# Example usage\\r\\nif __name__ == \"__main__\":\\r\\n    # Create the research chain\\r\\n    research_chain = create_research_chain()\\r\\n\\r\\n    # User input\\r\\n    user_input = \"Can you provide the latest research on quantum computing?\"\\r\\n\\r\\n    # Get the chatbot\\'s response\\r\\n    response = research_chain.run(human_input=user_input)\\r\\n\\r\\n    # Print the response\\r\\n    print(response)\\r\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageState(TypedDict):\n",
    "    generate_query: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyzeQuery(BaseModel):\n",
    "    \"\"\"Determines whether the given input is related to blog generation. If the query is about creating, drafting, or generating blog content, return 'yes'. If it pertains to any other topic, return 'no'.\"\"\"\n",
    "    binary_score: Literal[\"yes\",\"no\"] = Field(\"Indicates whether the input is related to blog generation. Accepts 'yes' if relevant to blog creation; otherwise, 'no'.\")\n",
    "\n",
    "system = \"\"\"  \n",
    "You are an AI-powered query evaluator. Your task is to determine whether a given input query is specifically related to **blog generation**.  \n",
    "\n",
    "Relevance Criteria:  \n",
    "- **Blog Generation Focus** = The query should pertain to creating, drafting, or generating blog content.   \n",
    "- **Exclusion of Unrelated Topics** = Queries related to general search, news, technical issues, or non-blog content should be excluded.  \n",
    "\n",
    "You must return a binary score:  \n",
    "- **\"yes\"** → If the query is about blog generation.  \n",
    "- **\"no\"** → If the query is unrelated to blog creation. \n",
    "\"\"\"\n",
    "AnalyzeQueryLLM = llm.with_structured_output(AnalyzeQuery)\n",
    "AnalyzeQueryPrompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",system),\n",
    "    (\"human\",\"Analyze the query : {query}\")\n",
    "])\n",
    "\n",
    "AnalyzeQueryChain = (AnalyzeQueryPrompt | AnalyzeQueryLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzeQuery(binary_score='no')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnalyzeQueryChain.invoke({\"query\":\"Hi How are you ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateBlog:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_phidata_agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
